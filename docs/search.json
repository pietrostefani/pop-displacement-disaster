[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Dynamic Estimates of Displacement in Disaster Regions",
    "section": "",
    "text": "Welcome\nThis website hosts the materials for the final workshop & hackaton of Project: Dynamic Estimates of Displacement in Disaster Regions: A Policy-Driven Framework Triangulating Data by Dr.¬†Elisabetta Pietrostefani, Matt Mason and Prof.¬†Francisco Rowe from the University of Liverpool and Hong Tran-Jones from IOM UN Migration - Displacement Tracking Matrix.\nConducted in collaboration with the International Organization for Migration‚Äôs Displacement Tracking Matrix (DTM) and Snowflake Inc., this project explores innovative ways to estimate displacement in disaster-affected regions. By comparing digital trace data with traditional data sources, it aims to develop a robust, policy-driven framework for triangulating and enhancing displacement estimates.\nThe website is licensed under the Attribution-NonCommercial-NoDerivatives 4.0 International.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Dynamic Estimates of Displacement in Disaster Regions",
    "section": "Contact",
    "text": "Contact\n\nElisabetta Pietrostefani - e.pietrostefani [at] liverpool.ac.uk Lecturer in Geographic Data Science, University of Liverpool.",
    "crumbs": [
      "Welcome"
    ]
  },
  {
    "objectID": "overview.html",
    "href": "overview.html",
    "title": "Overview",
    "section": "",
    "text": "The global challenge of internal displacement, exacerbated by conflict, climate-induced natural hazards and disasters, requires innovative and collaborative approaches to ensure effective responses. As of 2025, displacement continues to surge globally‚Äîwith 3.7 million people uprooted within Ukraine due to conflict, 18.8 million internally displaced across the Horn of Africa from a mix of drought and violence, and millions more affected annually in South and Southeast Asia, where disasters like cyclones triggered at least 1.8 million displacements in 2023 alone. These figures are a stark reminder of the need for timely, accurate, and spatially detailed data and accessible ways to process it to inform humanitarian action and policy.\nThis project‚Äîled by the Geographic Data Science Lab at the University of Liverpool, IOM‚Äôs Displacement Tracking Matrix and Snowflake‚Äôs End Data Disparity initiative ‚Äîcompares digital trace data with traditional sources to triangulate and improve displacement estimates.\nBy leveraging these diverse datasets, the project aims to enhance the precision and reliability of displacement estimates.\nUsing Ukraine as an initial case study, this approach illustrates the ability to generate actionable insights in diverse and complex settings requiring rapid data insights, policy and humanitarian responses. During the hackathon, the framework will be applied to a real-world climate-induced hazard scenario‚Äîthe Pakistan floods of August 2022‚Äîto support timely, data-driven disaster response efforts.\nThis work has been developed in close partnership with operational agencies, ensuring its relevance to real-world needs and its potential to inform evidence-based decision-making.",
    "crumbs": [
      "Overview"
    ]
  },
  {
    "objectID": "hackathon.html",
    "href": "hackathon.html",
    "title": "Hackathon",
    "section": "",
    "text": "Building on work by the University of Liverpool and IOM using digital data to track displacement in conflict settings, the Hackathon explores how similar approaches can be applied in the context of climate-induced natural hazards. In the aftermath of such disasters, understanding internal displacement patterns is vital for effective humanitarian response. While traditional survey-based methods, like those used by IOM, offer valuable insights in informing operational responses to mitigate the effect of disasters, they often take time to develop and be fully operational. Digital-trace data present opportunities for more timely, detailed, and dynamic tracking of population movements before and after climate events, to inform rapid response operations and support spatially targeted interventions.\nDuring the Hackathon, participants will utilise Meta‚Äôs ‚ÄúFacebook Movement During Crisis‚Äù and ‚ÄúFacebook Population During Crisis‚Äù data derived from mobile phone location records, alongside IOM‚Äôs displacement data. The Facebook Movement dataset records the movements of Facebook users, showing how people travel between different areas during a crisis. It includes both the typical movement patterns observed before the crisis and the actual movements during the event, allowing for a comparison between the two. The Population dataset includes information about the number of active Facebook users in a specific area during a crisis, the typical number of active users in that area before the crisis, and the difference between these two values. By integrating these datasets with traditional sources and socio-economic variables, teams will explore innovative approaches to enhance disaster response, optimise resource allocation, and develop predictive models for future crises.",
    "crumbs": [
      "Hackathon"
    ]
  },
  {
    "objectID": "casestudy.html",
    "href": "casestudy.html",
    "title": "Case-study",
    "section": "",
    "text": "Pakistan ‚Äì Flood Response Community Needs Identification (CNI)\nThe specific case study for this Hackathon will be the floods that struck Pakistan in 2022. Between June and October of that year, unprecedented monsoon rains affected over 33 million people across the country, resulting in 1,739 deaths and an estimated $40 billion in property damage.\nThis event was selected for its large-scale humanitarian and economic impact, and the availability of high-quality data from both the International Organization for Migration (IOM) and Meta. These datasets provide an opportunity for participants to compare, combine, and analyse real-world information in a meaningful way.\nFocusing on a disaster aligns with the Hackathon‚Äôs broader goal: addressing challenges that are becoming increasingly frequent due to climate change. Floods, in particular, represent a recurring threat in Pakistan, making this case both historically relevant and urgently contemporary and is one of the types of disaster events recording the greatest frequency worldwide.\nAn example of the locations assessed by IOM in Round 6 ‚Äì Balochistan and Khyber Pakhtunkhwa Provinces (August 2024).",
    "crumbs": [
      "Hackathon",
      "Case-study"
    ]
  },
  {
    "objectID": "datasets.html",
    "href": "datasets.html",
    "title": "Data",
    "section": "",
    "text": "1. Data Access\nThis section of the webpage briefly outlines the datasets that should be used during the Hackathon and provides related resources and documentation. The Hackathon‚Äôs problem statement can be found here and should be referred to regarding the event‚Äôs purpose and objectives.\nThe human mobility datasets provided for the Hackathon fall into one of two categories: digital data (from Meta) or key informant survey data (from IOM). The table below outlines these, summarising their accessibility, processing and stability. In addition, contextual data on Pakistan is provided to help with analysis, as well as a variety of spatial boundaries that can be used to map the provided datasets. Each dataset is outlined below, alongside any available documentation and guidance for analysis.\nAll the data mentioned in this document has been uploaded onto Snowflake. As part of the Hackathon, Snowflake colleagues will provide a tutorial on how to access and analyse the data in Snowflake. They will be available throughout the event to answer any queries and a series of guides on using Snowflake can be found on this section of the webpage.",
    "crumbs": [
      "Hackathon",
      "Data"
    ]
  },
  {
    "objectID": "datasets.html#iom-dtm",
    "href": "datasets.html#iom-dtm",
    "title": "Data",
    "section": "IOM DTM",
    "text": "IOM DTM",
    "crumbs": [
      "Hackathon",
      "Data"
    ]
  },
  {
    "objectID": "output.html",
    "href": "output.html",
    "title": "Output",
    "section": "",
    "text": "Pre-Hackathon Meetings\nTwo pre-Hackathon meeting will take place online.\nThis session will cover:",
    "crumbs": [
      "Hackathon",
      "Output"
    ]
  },
  {
    "objectID": "output.html#pre-hackathon-meetings",
    "href": "output.html#pre-hackathon-meetings",
    "title": "Output",
    "section": "",
    "text": "Team Formation & Problem Selection: Participants meet their teammates and choose a problem statement to work on.\nPre-Hackathon Demo (Snowflake)\nüìÖ Friday, 16 May\nüïö 11:00 AM (BST)\n\n\n\nGain access to the datasets\nLearn how to connect to Snowflake accounts and locate datasets and demo code.\nIntroduction to Snowflake‚Äôs geospatial analysis capabilities\nUsing Snowflake Notebooks (similar to working in Jupyter) to build demographic, time-series, and geospatial infographics\nExample of templates for AI-generated summaries and AI-driven insights between demographic and geospatial data\nFor those preferring to work locally, a Jupyter Notebook template for connecting to Snowflake will also be shared.",
    "crumbs": [
      "Hackathon",
      "Output"
    ]
  },
  {
    "objectID": "output.html#what-groups-should-keep-in-mind",
    "href": "output.html#what-groups-should-keep-in-mind",
    "title": "Output",
    "section": "What Groups should keep in mind",
    "text": "What Groups should keep in mind\nParticipants will focus on triangulating key indicators of displacement and presenting their findings in clear, accessible formats that can inform both policymakers and humanitarian responders. The goal is to develop a compelling ‚ÄúMessage + Infographic (Map or Other)‚Äù‚Äîa concise narrative supported by data visualisation ‚Äîthat can serve as a proof of concept for improving disaster response in future scenarios.\n\n\n\n\n\n\nInfographic and Code Guidance\n\n\n\n\nInfographic: Most effective visualisations are usually a combination of images: maps + plots + icons, etc.\nCode: Teams are strongly encouraged to submit any code developed during the Hackathon. This may include the creation of a GitHub repository to share scripts, data pipelines, or analysis workflows. Open and well-documented code will help ensure the solutions are reusable and scalable for future disaster response efforts.\n\nüõ†Ô∏è Note: UoL may implement the proposed indicators into a software package and is eager to take the lead in developing this package as part of a collaborative effort.\n\n\nThis Hackathon offers an opportunity to contribute to tools and strategies that could significantly improve disaster response and resilience, not just in Pakistan but globally.",
    "crumbs": [
      "Hackathon",
      "Output"
    ]
  },
  {
    "objectID": "output.html#output-to-be-submitted",
    "href": "output.html#output-to-be-submitted",
    "title": "Output",
    "section": "Output to be submitted",
    "text": "Output to be submitted\n\n\n\n\n\n\nPresentation and Code Submission\n\n\n\nEach team should prepare a concise presentation (around 5 slides) to showcase their work at the end of the Hackathon. Focus on clear and compelling communication of your findings‚Äîimpact matters as much as accuracy.\nIn addition, teams are strongly encouraged to submit any code developed during the Hackathon. Sharing your code (e.g., via a GitHub repository) helps ensure that your solutions are reproducible, scalable, and ready to support future disaster response efforts.",
    "crumbs": [
      "Hackathon",
      "Output"
    ]
  },
  {
    "objectID": "researchareas.html",
    "href": "researchareas.html",
    "title": "Problem Statements",
    "section": "",
    "text": "1. Event-Based or Rapid Response: Developing Key Indicators\nParticipants must select one of the following research areas to focus on during the Hackathon. Each group is required to confirm their chosen area during the pre-event meeting, which will be held the Week of May 12th.\nAll teams are required to use at least one of the provided datasets (e.g., Meta, IOM). Given the limited timeframe of the Hackathon, the use of additional datasets is generally discouraged; however, if you have a specific data source in mind that supports your concept, please consult the organisers in advance. The organising team is available to answer any questions and provide further guidance in the lead-up to the event.\nFocus on generating timely, actionable indicators from Facebook and IOM data to support immediate disaster response operations.",
    "crumbs": [
      "Hackathon",
      "Problem Statements"
    ]
  },
  {
    "objectID": "researchareas.html#event-based-or-rapid-response-developing-key-indicators",
    "href": "researchareas.html#event-based-or-rapid-response-developing-key-indicators",
    "title": "Problem Statements",
    "section": "",
    "text": "Develop key indicators from Meta data such as displacement rates, volume of movement, identification of vulnerable populations, or most affected regions.\nExplore how these indicators compare with‚Äîor can be enriched by‚ÄîIOM data collected in reference to the 2022 flooding event in Pakistan.\nWhat aspects of local population, their movements and geographical context are most useful to humanitarian responders in the first hours and days of a disaster?",
    "crumbs": [
      "Hackathon",
      "Problem Statements"
    ]
  },
  {
    "objectID": "researchareas.html#learning-from-the-past-patterns-and-resilience",
    "href": "researchareas.html#learning-from-the-past-patterns-and-resilience",
    "title": "Problem Statements",
    "section": "2. Learning from the Past: Patterns and Resilience",
    "text": "2. Learning from the Past: Patterns and Resilience\nAnalyse data to identify behavioural trends and resilience patterns from the 2022 floods to today (IOM Pakistan Flood Responses Rounds 1-6).\n\nAre there repeated behavioural patterns in terms of population movement, key origins, key destinations and population distribution across different flood events in Pakistan?\nWhich regions show higher levels of resilience, and what factors contribute to that resilience (e.g., infrastructure, demographics, preparedness)?\nUse comparative analysis to highlight learnings that can inform future planning and interventions.",
    "crumbs": [
      "Hackathon",
      "Problem Statements"
    ]
  },
  {
    "objectID": "researchareas.html#predicting-future-movement-mobility-modelling",
    "href": "researchareas.html#predicting-future-movement-mobility-modelling",
    "title": "Problem Statements",
    "section": "3. Predicting Future Movement: Mobility Modelling",
    "text": "3. Predicting Future Movement: Mobility Modelling\nDevelop predictive models to forecast displacement patterns in future disaster scenarios by leveraging social, economic, and geographic indicators, using data from the International Organization for Migration (IOM) and Facebook (Meta) mobility data.\n\nWhat kinds of areas are most likely to receive displaced populations? (more vulnerable communities?)\nCan we identify a systematic set of predictors for where people move from and to during disasters?\nHow can the information extracted from these analyses be used to improve preparedness and resource allocation in high-risk regions and destination community areas.",
    "crumbs": [
      "Hackathon",
      "Problem Statements"
    ]
  },
  {
    "objectID": "researchareas.html#relationship-between-flood-severity-extent-of-displacement",
    "href": "researchareas.html#relationship-between-flood-severity-extent-of-displacement",
    "title": "Problem Statements",
    "section": "4. Relationship between Flood Severity & Extent of Displacement",
    "text": "4. Relationship between Flood Severity & Extent of Displacement\n\nHow are flood severity metrics associated with higher displacement volumes or longer displacement durations?\nCan threshold points in flood impact metrics be identified? That is points beyond which displacement becomes significantly more likely\nHow do displacement outcomes differ in high-severity vs.¬†low-severity flood zones?",
    "crumbs": [
      "Hackathon",
      "Problem Statements"
    ]
  },
  {
    "objectID": "datasets.html#datasets",
    "href": "datasets.html#datasets",
    "title": "Data",
    "section": "2. Datasets",
    "text": "2. Datasets\n\n2.1. Facebook Population and Movement During Crisis\n\nMeta provides these data as part of their Data for Good programme in the aftermath of humanitarian disasters. Once uploaded, these data are available to researchers and policymakers for 90 days before being removed from the Data for Good platform. The data shows the number of Facebook users located within a given spatial unit at a given time.\nFor the Hackathon, we are using the data available in the immediate aftermath of the 2022 Pakistan floods. The data refer to the period 14 August 2022 ‚Äì 7 September 2022 and are at two geographic scales: 800m tiles (known as quadkeys, based on the Bing Maps tile system) and aggregated to global administrative (GADM) level 2 geographies.\nThese data consist of two datasets, Population During Crisis and Movement During Crisis:\n\nPopulation ‚Äì these are population stock data, showing the number of users in each spatial unit at three snapshots: 00:00, 8:00 and 16:00 (Pacific Time). The data are removed when there are fewer than 10 observations.\n\nSnowflake Schema: META_DATA_FOR_GOOD\nSnowflake Views: ADMIN_L2_AGGREGATED_POPULATION and TILE_POPULATION\n\nMovement ‚Äì these are population flow data, showing the origin and destination of users between temporal points. Users‚Äô origin and destination are chosen according to where they spent most time within each 8-hour window. For example, data recorded at 16:00 shows the flow between areas from 08:00 to 16:00. Where there are fewer than 10 observations for a flow, data are removed. The Pakistan data available for the Movement data are only at the 08:00 and 16:00 time periods, but not for the 00:00 period.\n\nSnowflake Schema: META_DATA_FOR_GOOD\nSnowflake Views: ADMIN_L2_AGGREGATED_MOVEMENT and TILE_MOVEMENT\n\n\nThe data are generally comprehensive, but there are some gaps, largely due to collection issues at Meta‚Äôs end. For example, in both the tiled and aggregated Movement data, only the 08:00 and 16:00 time stamps are available, with the 00:00 period missing. Additionally, the aggregated Population data for 20, 21 and 22 August are also missing.\nBoth datasets contain data from a baseline period before the disaster event to compare users‚Äô stock or flow during the crisis. The raw and percentage differences are provided within the dataset, along with a z-score to assess the statistical significance of the change from the pre-crisis baseline to the crisis period.\nThe data is available as a series of csv files. Each file is either the tiled or aggregated data for each time stamp of each day of the Population or Movement data.\n\nA guide on using these data in R can be found here.\n\n\n2.2. IOM Data\n\nThese data are collated and published by the IOM‚Äôs Displacement Tracking Matrix (DTM) team. The primary data produced for Pakistan by the IOM in the context of the 2022 floods are the Flood Response Community Needs Identification (CNI) datasets. These data are collated and published in rounds; six rounds have been run so far, though data for Round 5 is not publicly available.\nThe data for all 5 publicly available rounds can be found in Snowflake Schema: IOM_CNI_DATA\nData from the CNI is derived from key informant interviews and direct observations. It provides estimates for the number of temporarily displaced persons (TDPs), the number of returnees and other variables related to displaced populations for each village surveyed. A more detailed methodology for these data can be found in the reports below. These data are often very challenging to collect, and, as a result, each round does not cover the exact same areas, or collect the same level of data.\nThe data are provided at the village/settlement level. Also included are variables for province, district and Tehsil. For the first two, codes are provided also to make them easier to be joined to spatial boundaries. These codes are p-codes, a geographic framework commonly used by the Office for the Coordination of Humanitarian Affairs (OCHA) to link data to administrative areas. These geographies and codes do not map onto those that the Facebook Population and Movement data are aggregated to. A short guide on the different geographies is provided at the end of this guidance page.\nFor the Hackathon, each data round is provided. The data format and variable names vary slightly between rounds, though each generally provides similar variables and data. Alongside each round of data, a report is also published. Reports can be found here for each round:\n\nRound 1 (Nov ‚Äì Dec 2022)\nRound 2 (Jan ‚Äì Mar 2023)\nRound 3 ‚Äì Balochistan Province (May ‚Äì June 2023)\nRound 3 ‚Äì Khyber Pakhtunkhwa Province (May ‚Äì June 2023)\nRound 3 ‚Äì Sindh Province (May ‚Äì June 2023)\nRound 4 ‚Äì Balochistan Province (August 2023)\nRound 4 ‚Äì Khyber Pakhtunkhwa Province (August 2023)\nRound 4 ‚Äì Sindh Province (August 2023)\nRound 6 ‚Äì Balochistan and Khyber Pakhtunkhwa Provinces (August 2024)\n\n\n\n\n2.3. Contextual Data\n\nAlongside mobility data, we have prepared a series of datasets that provide additional context to Pakistan. They consist of population data in raster and aggregated form, and socioeconomic data, also in raster and aggregated format.\nRaster population data:\n\nPopulation estimates for Pakistan for 2020 from WorldPop. The data is at 100m resolution grids in a single raster file.\nPopulation estimates for Pakistan by age and sex for 2020 from WorldPop. The data is at 100m resolution grids in a series of rasters. Files are structured like {iso}¬†{gender}¬†{age group}¬†{year}¬†{type}¬†{resolution}.tif - gender fields are f (female), m (male), t (total); age group fields are 00 (0-12 months), 01 (1-4 years), 05 (5-9 years) and so on until 90 (age 90 and above).\n\nSnowflake Schema: CONTEXTUAL_DATA\nSnowflake View: POPULATION_BY_SEX_BY_AGE_GROUP_100MTR_RESOLUTION\n\n\nAggregated population data:\n\nPopulation estimates for Pakistan for 2020 from the WorldPop raster, aggregated (total_pop) to global administrative level 2, and the province and district polygons used by OCHA. The data is calculated by the project team using WorldPop data and spatial polygons provided by GADM and OCHA.\n\nSnowflake Schema: CONTEXTUAL_DATA\nSnowflake Views: POPULATION_PAKISTAN_2020_*\n\n\nSocioeconomic data:\n\nDeprivation data from The Global Gridded Relative Deprivation Index (GRDI). The data is a raster file at 1km resolution, cropped to Pakistan. Data are on a 0-100 scale, with high values indicating higher relative levels of deprivation. Complete documentation for the data can be accessed here. These have also been aggregated to global administrative level 2 and the province and district polygons used by OCHA by computing the mean value (mean_rdi) within the polygon from the GRDI raster data.\n\nSnowflake Schema: CONTEXTUAL_DATA\nSnowflake Views: GRDI_*\n\n\nClimate data:\n\nSatellite detected water extents in Pakistan between 01 and 29 August 2022. The data is a series of shapefiles showing flood extent based on satellite imagery. Data is from the UN Operational Satellite Applications Programme (UNOSAT), found here.\nSentinel-1 based analysis of Pakistan floods, capturing flood progression between 10 August and 23 September 2022. Data is sourced from TU Wein and is compromised of two raster layers at 20-metre resolution: first_flood_detection.tif, the day the flood was first detected, recorded as day-of-year (DOY), and flood_frequency.tif, the percentage of valid observations in which a flood was detected (a proxy for flood permanence).\nDekadal (10-day) rainfall data from 2021 onward at the district level (coded to OCHA administrative level 2) sourced from the World Food Programme (WFP). Data shows series of metrics for rainfall, listed here.\nDekadal (10-day) Normalized Difference Vegetation Index (NDVI) data from 2021 onward at the district level (coded to OCHA administrative level 2) sourced from the World Food Programme (WFP). Data shows series of metrics for vegetation greenness, listed here, normally used to quantify the health and density of vegetation.\nSnowflake Data Access:\n\nSnowflake Schema: CONTEXTUAL_DATA\nSnowflake Views: CLIMATE_PAKISTAN_*\n\n\n\n\n\n2.4. Spatial Boundaries\n\nDue to recent boundary changes and differing administrative boundaries being used by different organisations, the joining of spatial data together for Pakistan is not a simple task. Much of the data listed above, however, is able to be joined to a spatial boundary to be mapped and analysed. This section describes the spatial boundaries made available, as well a table with details of how the Meta, IOM and contextual datasets can be joined to these.\nThe boundaries available are:\n\nSpatial polygons for global administrative levels, sourced from GADM. The shapefile boundaries for global administrative levels 0, 1, 2 and 3 and the geopackage for Pakistan are included. These boundaries can be joined to the aggregated Facebook Population and Movement data, as well as the population and deprivation data aggregated to global administrative level 2.\n\nSnowflake Schema: SPATIAL_BOUNDARIES\nSnowflake Views: PAKISTAN_GADM_ADMIN_LEVEL_POLYGONS\n\nSpatial polygons used by OCHA for subnational boundaries of Pakistan. Data is shapefiles for provinces (adm1), districts (adm2) and tehsils (adm3), sourced from OCHA. These boundaries can be joined to the spatial variable codes in the IOM CNI data for provinces and districts, as well as the population and deprivation data aggregated to OCHA administrative level 1 and 2, and to the WFP climate data.\n\nSnowflake Schema: SPATIAL_BOUNDARIES\nSnowflake Views: PAKISTAN_OCHA_ADMIN_LEVEL_POLYGONS\n\nSpatial polygons for the 800m tiles (also known as quadkeys) found in the Facebook Population and Movement tile data. These were created from the tile datasets using the R package quadkeyr.\n\nSnowflake Schema: SPATIAL_BOUNDARIES\nSnowflake Views: META_QUADKEY_POLYGONS\n\n\nThe below table provides an overview of the variables in each dataset and which boundary they relate to.\n\n\n\n\n\n\n\n\n\n\nDataset\nSpatial Codes\nType\nBoundary File and Corresponding Code\n\n\n\n\nFacebook Data\n\n\n\n\n\nFacebook Population (aggregated)\npolygon_id\nGADM level 2 Polygon\nGlobal administrative level 2 polygons (GID_2)\n\n\nFacebook Population (tiled)\nquadkey\n800m tile\nQuadkey polygons (quadkey)\n\n\nFacebook Movement (aggregated)\nstart_polygon_id\nend_polygon_id\nOrigin polygon\nDestination polygon\nGlobal administrative level 2 polygons (GID_2)\n\n\nFacebook Movement (tiled)\nstart_quadkey\nend_quadkey\nOrigin 800m tile\nDestination 880m tile\nQuadkey polygons (quadkey)\n\n\nIOM Data\n\n\n\n\n\nIOM CNI Data\nProvinceCode/Pcode/PCODE; or Admin1 pcode\nDistrict Code/Pcode/PCODE; or Admin2 pcode\nProvince (adm1) polygon\nDistrict (adm2) polygon\nOCHA subnational polygons (ADM1_PCODE) (ADM2_PCODE)\n\n\nContextual Data\n\n\n\n\n\nAggregated Population Data (gadm_2)\nGID_2\nGADM level 2 Polygon\nGlobal administrative level 2 polygons (GID_2)\n\n\nAggregated Population Data (ocha_1 and ocha_2)\nADM1_PCODE\nADM2_PCODE\nProvince (adm1) polygon\nDistrict (adm2) polygon\nOCHA subnational polygons (ADM1_PCODE)(ADM2_PCODE)\n\n\nAggregated Global Relative Deprivation Index Data (GRDI) (gadm_2)\nGID_2\nGADM level 2 Polygon\nGlobal administrative level 2 polygons (GID_2)\n\n\nAggregated Global Relative Deprivation Index Data (GRDI) (ocha_1 and ocha_2)\nADM1_PCODE\nADM2_PCODE\nProvince (adm1) polygon\nDistrict (adm2) polygon\nOCHA subnational polygons (ADM1_PCODE)(ADM2_PCODE)\n\n\nWFP - Rainfall at Subnational Level\nADM2_PCODE\nDistrict (adm2) polygon\nOCHA subnational polygons (ADM2_PCODE)\n\n\nWFP - NVDI at Subnational Level\nADM2_PCODE\nDistrict (adm2) polygon\nOCHA subnational polygons (ADM2_PCODE)",
    "crumbs": [
      "Hackathon",
      "Data"
    ]
  },
  {
    "objectID": "dataaccess.html",
    "href": "dataaccess.html",
    "title": "Data Access",
    "section": "",
    "text": "Access your Snowflake account\nTo access your Snowflake account follow the instructions on this link that will provide you with a temporary account and access to the data for the Hackathon.",
    "crumbs": [
      "Snowflake",
      "Data Access"
    ]
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "Whether you‚Äôre here before the hackathon to sharpen your skills, or you‚Äôre looking for some pre-prepared code snippets to start from, you‚Äôve come to the right place!\n\nSnowflake Quickstarts\nQuickstarts are exactly that, follow along tutorials with step by step walk-throughs - perfect for using as template code:\n\nTasty Bytes - Zero to Snowflake - Geospatial\nGetting started with Geospatial - Geography\nGeospatial Analysis of Precisely Datasets using Snowflake Notebooks and Streamlit\nUsing Snowflake Cortex (AI) and Streamlit with Geospatial Data\nSnowpark Intro\n\n\n\nSnowflake Docs\nHere are some useful places to look before and during the hackathon:\n\nGeospatial Data Types\nGeospatial Functions\nSnowpark Dataframe developer reference\n\n\n\nDemos and more content\nCheck out lab instructions on this event page for some handpicked content",
    "crumbs": [
      "Snowflake",
      "Resources"
    ]
  },
  {
    "objectID": "snowflake.html",
    "href": "snowflake.html",
    "title": "Snowflake",
    "section": "",
    "text": "Snowflake is a cloud-native data and AI platform that provides data warehousing and analytics. Its architecture separates data storage from computing resources, allowing users to optimize costs by paying only for usage. This structure supports varied data applications, including data warehousing, data engineering, AI/ML modeling, fine-tuning, and inference. Snowflake operates across major public cloud providers (Amazon Web Services, Microsoft Azure, and Google Cloud Platform), offering multi-cloud flexibility and streamlined data sharing.\nSnowflake has many programs and activities aimed at community engagement and involvement. Data for Good is a key initiative within the European Marketing team, focused on mobilizing data to address global challenges. It supports non-profit organizations, academic institutions, and public sector bodies by offering access to Snowflake‚Äôs platform, resources, and data expertise. The program aims to empower these entities to harness data for social and environmental impact, contributing to solutions for issues like climate change, public health, and humanitarian efforts.\nWithin its social impact efforts, Snowflake has a new initiative called ‚ÄúEnd Data Disparity.‚Äù Disparity refers to lack of equal representation within datasets, and gaps in data access, analytics, and data collaboration. Snowflake‚Äôs objective in this area is to broaden data accessibility and literacy, enabling more communities and organizations, especially those that are underserved, to benefit from data-driven insights. By providing technology and support, Snowflake aims to help level the playing field, allowing these groups to use data to advocate for their needs, improve services, and foster equitable outcomes.\nThrough its platform architecture and dedicated social impact programs, Snowflake seeks to expand the reach and benefits of data analytics. By fostering data collaboration and aiming to reduce disparities in data access and capability, Snowflake seeks to enable both technological advancement and positive societal change.",
    "crumbs": [
      "Snowflake"
    ]
  },
  {
    "objectID": "workshop.html",
    "href": "workshop.html",
    "title": "Workshop",
    "section": "",
    "text": "Day 1 - May 21st 2025\nA workshop fostering dialogue with key international stakeholders focusing on key added value and limitations of digital trace data for humanitarian action with illustrations. The workshop will be followed by a hackathon where teams use displacement estimates derived from digital trace data for humanitarian action.",
    "crumbs": [
      "Workshop"
    ]
  },
  {
    "objectID": "workshop.html#section",
    "href": "workshop.html#section",
    "title": "Workshop",
    "section": "",
    "text": "9:00‚Äì9:30 ‚Äì Welcome, Introductions & Agenda ‚Äì Prof.¬†Francisco Rowe (UoL)\n9:30‚Äì9:50 ‚Äì Assessing Non-traditional Data Sources ‚Äì Kate Hokinson (OCHA)\n\n9:50‚Äì10:10 ‚Äì Data and findings from the project ‚Äì Dr.¬†Elisabetta Pietrostefani (UoL)\n10:10‚Äì10:30 ‚Äì Lessons learned from the project Digital Trace Data for Population Displacement ‚Äì Matt Mason (UoL)\n10:30‚Äì10:45 ‚Äì Questions & Discussion\n\n\n10:45‚Äì11:00 ‚Äì Coffee Break",
    "crumbs": [
      "Workshop"
    ]
  },
  {
    "objectID": "workshop.html#section-1",
    "href": "workshop.html#section-1",
    "title": "Workshop",
    "section": "11:00‚Äì13:00",
    "text": "11:00‚Äì13:00\n\n11:00‚Äì11:20 ‚Äì Using Facebook Movement During Crisis ‚Äì Dr.¬†Carmen Cabrera-Arnau & Prof.¬†Francisco Rowe\n\n11:20‚Äì11:30 ‚Äì Hackathon Problem Statements - Hong Tran-Jones (IOM DTM)\n11:30‚Äì11:50 ‚Äì Facebook Population and Movement Data ‚Äì Matt Mason (UoL)\n11:50‚Äì12:10 ‚Äì IOM Pakistan Data ‚Äì Ellen Van de Weghe (IOM Pakistan)\n12:10‚Äì12:40 ‚Äì Accessing Your Data and Creating an Infographic: A Short Demo using Hackathon Data ‚Äì Euan Newlands (Snowflake)\n12:40‚Äì13:00 ‚Äì Questions & Discussion",
    "crumbs": [
      "Workshop"
    ]
  },
  {
    "objectID": "workshop.html#day-1",
    "href": "workshop.html#day-1",
    "title": "Workshop",
    "section": "",
    "text": "Joining Online ‚Äì May 21st (Day 1), 9:00‚Äì13:00\nParticipants can join the workshop remotely via Microsoft Teams.\nJoin the meeting now: Click here to join\nMeeting ID: 311 232 317 588\nPasscode: eY2xq6eD\nIf you need assistance, please reach out to the event organizers.\n\n\n9:00‚Äì11:00\n\n9:00‚Äì9:30 ‚Äì Welcome, Introductions & Agenda ‚Äì Prof.¬†Francisco Rowe (UoL)\n9:30‚Äì9:50 ‚Äì Assessing Non-traditional Data Sources ‚Äì Kate Hokinson (OCHA)\n\n9:50‚Äì10:10 ‚Äì Data and findings from the project ‚Äì Dr.¬†Elisabetta Pietrostefani (UoL)\n10:10‚Äì10:30 ‚Äì Lessons learned from the project Digital Trace Data for Population Displacement ‚Äì Matt Mason (UoL)\n10:30‚Äì10:45 ‚Äì Questions & Discussion\n\n\n10:45‚Äì11:00 ‚Äì Coffee Break\n\n\n\n11:00‚Äì13:00\n\n11:00‚Äì11:20 ‚Äì Using Facebook Movement During Crisis ‚Äì Dr.¬†Carmen Cabrera-Arnau & Prof.¬†Francisco Rowe\n\n11:20‚Äì11:30 ‚Äì Hackathon Problem Statements - Hong Tran-Jones (IOM DTM)\n11:30‚Äì11:50 ‚Äì Facebook Population and Movement Data ‚Äì Matt Mason (UoL)\n11:50‚Äì12:10 ‚Äì IOM Pakistan Data ‚Äì Ellen Van de Weghe (IOM Pakistan)\n12:10‚Äì12:40 ‚Äì Accessing Your Data and Creating an Infographic: A Short Demo using Hackathon Data ‚Äì Euan Newlands (Snowflake)\n12:40‚Äì13:00 ‚Äì Questions & Discussion\n\n\n\n13:00‚Äì14:00\nNetworking Lunch\n\n\n14:00‚Äì17:00\n\nHackathon Session 1\n\n\n\n17:00‚Äì18:00\n\nTeam Progress Updates: Goals and Objectives\n\n\n\nEvening\n18:30‚Äì21:00 ‚Äì Networking Dinner",
    "crumbs": [
      "Workshop"
    ]
  },
  {
    "objectID": "workshop.html#day-2",
    "href": "workshop.html#day-2",
    "title": "Workshop",
    "section": "Day 2",
    "text": "Day 2\n\n9:00‚Äì9:30\n\nRecap of Hackathon Objectives and Goals - Prof.¬†Francisco Rowe\n\n\n\n9:00‚Äì11:00\n\nHackathon Session 2\n\n\n10:45‚Äì11:00 ‚Äì Coffee Break\n\n\n\n11:00‚Äì13:00\n\nHackathon Session 3\n\n\n\n13:00‚Äì14:00\nWorking Lunch\n\n\n14:00‚Äì17:00\n\nHackathon Final Session\n\n\n\n17:00‚Äì18:00\n\nTeam Presentations to Panel of Judges",
    "crumbs": [
      "Workshop"
    ]
  },
  {
    "objectID": "workshop.html#joining-online-may-21st-day-1-9001300",
    "href": "workshop.html#joining-online-may-21st-day-1-9001300",
    "title": "Workshop",
    "section": "üîó Joining Online ‚Äì May 21st (Day 1), 9:00‚Äì13:00",
    "text": "üîó Joining Online ‚Äì May 21st (Day 1), 9:00‚Äì13:00\nParticipants can join the workshop remotely via Microsoft Teams.\nJoin the meeting now: Click here to join\nMeeting ID: 311 232 317 588\nPasscode: eY2xq6eD\nIf you need assistance, please reach out to the event organizers.\n\n9:00‚Äì11:00\n\n9:00‚Äì9:30 ‚Äì Welcome, Introductions & Agenda ‚Äì Prof.¬†Francisco Rowe (UoL)\n9:30‚Äì9:50 ‚Äì Assessing Non-traditional Data Sources ‚Äì Kate Hokinson (OCHA)\n\n9:50‚Äì10:10 ‚Äì Data and findings from the project ‚Äì Dr.¬†Elisabetta Pietrostefani (UoL)\n10:10‚Äì10:30 ‚Äì Lessons learned from the project Digital Trace Data for Population Displacement ‚Äì Matt Mason (UoL)\n10:30‚Äì10:45 ‚Äì Questions & Discussion\n\n\n10:45‚Äì11:00 ‚Äì Coffee Break\n\n\n\n11:00‚Äì13:00\n\n11:00‚Äì11:20 ‚Äì Using Facebook Movement During Crisis ‚Äì Dr.¬†Carmen Cabrera-Arnau & Prof.¬†Francisco Rowe\n\n11:20‚Äì11:30 ‚Äì Hackathon Problem Statements - Hong Tran-Jones (IOM DTM)\n11:30‚Äì11:50 ‚Äì Facebook Population and Movement Data ‚Äì Matt Mason (UoL)\n11:50‚Äì12:10 ‚Äì IOM Pakistan Data ‚Äì Ellen Van de Weghe (IOM Pakistan)\n12:10‚Äì12:40 ‚Äì Accessing Your Data and Creating an Infographic: A Short Demo using Hackathon Data ‚Äì Euan Newlands (Snowflake)\n12:40‚Äì13:00 ‚Äì Questions & Discussion\n\n\n\n13:00‚Äì14:00\nNetworking Lunch\n\n\n14:00‚Äì17:00\n\nHackathon Session 1\n\n\n\n17:00‚Äì18:00\n\nTeam Progress Updates: Goals and Objectives\n\n\n\nEvening\n18:30‚Äì21:00 ‚Äì Networking Dinner",
    "crumbs": [
      "Workshop"
    ]
  },
  {
    "objectID": "workshop.html#day-1---may-21st-2025",
    "href": "workshop.html#day-1---may-21st-2025",
    "title": "Workshop",
    "section": "",
    "text": "Joining Online ‚Äì 9:00‚Äì13:00\nParticipants can join the workshop remotely via Microsoft Teams.\nJoin the meeting now: Click here to join\nMeeting ID: 311 232 317 588\nPasscode: eY2xq6eD\nIf you need assistance, please reach out to the event organizers.\n\n\n9:00‚Äì11:00\n\n9:00‚Äì9:30 ‚Äì Welcome, Introductions & Agenda ‚Äì Prof.¬†Francisco Rowe (UoL)\n9:30‚Äì9:50 ‚Äì Assessing Non-traditional Data Sources ‚Äì Kate Hodkinson (OCHA)\n\n9:50‚Äì10:10 ‚Äì Data and findings from the project ‚Äì Dr.¬†Elisabetta Pietrostefani (UoL)\n10:10‚Äì10:30 ‚Äì Lessons learned from the project Digital Trace Data for Population Displacement ‚Äì Matt Mason (UoL)\n10:30‚Äì10:45 ‚Äì Questions & Discussion\n\n\n10:45‚Äì11:00 ‚Äì Coffee Break\n\n\n\n11:00‚Äì13:00\n\n11:00‚Äì11:20 ‚Äì Using Facebook Movement During Crisis ‚Äì Dr.¬†Carmen Cabrera & Prof.¬†Francisco Rowe\n\n11:20‚Äì11:30 ‚Äì Hackathon Problem Statements - Hong Tran-Jones (IOM DTM)\n11:30‚Äì11:50 ‚Äì Hackathon Data ‚Äì Matt Mason (UoL)\n11:50‚Äì12:10 ‚Äì IOM Pakistan Data ‚Äì Ellen Van de Weghe (IOM Pakistan)\n12:10‚Äì12:40 ‚Äì Accessing Your Data and Creating an Infographic: A Short Demo using Hackathon Data ‚Äì Euan Newlands (Snowflake)\n12:40‚Äì13:00 ‚Äì Questions & Discussion\n\n\n\n13:00‚Äì14:00\nNetworking Lunch\n\n\n14:00‚Äì17:00\n\nHackathon Session 1\n\n\n\n17:00‚Äì18:00\n\nTeam Progress Updates: Goals and Objectives\n\n\n\nEvening\n18:30‚Äì21:00 ‚Äì Networking Dinner",
    "crumbs": [
      "Workshop"
    ]
  },
  {
    "objectID": "workshop.html#day-2-----may-22nd-2025",
    "href": "workshop.html#day-2-----may-22nd-2025",
    "title": "Workshop",
    "section": "Day 2 - - May 22nd 2025",
    "text": "Day 2 - - May 22nd 2025\n\n9:00‚Äì9:30\n\nRecap of Hackathon Objectives and Goals - Prof.¬†Francisco Rowe\n\n\n\n9:00‚Äì11:00\n\nHackathon Session 2\n\n\n10:45‚Äì11:00 ‚Äì Coffee Break\n\n\n\n11:00‚Äì13:00\n\nHackathon Session 3\n\n\n\n13:00‚Äì14:00\nWorking Lunch\n\n\n14:00‚Äì17:00\n\nHackathon Final Session\n\n\n\n17:00‚Äì18:00\n\nTeam Presentations to Panel of Judges",
    "crumbs": [
      "Workshop"
    ]
  },
  {
    "objectID": "probstatement.html",
    "href": "probstatement.html",
    "title": "Hackathon",
    "section": "",
    "text": "Building on work by the University of Liverpool and IOM using digital data to track displacement in conflict settings, the Hackathon explores how similar approaches can be applied in the context of climate-induced natural hazards. In the aftermath of such disasters, understanding internal displacement patterns is vital for effective humanitarian response.\nWhile traditional survey-based methods, like those used by IOM, offer valuable insights in informing operational responses to mitigate the effect of disasters, they often take time to develop and be fully operational. Digital-trace data present opportunities for more timely, detailed, and dynamic tracking of population movements before and after climate events, to inform rapid response operations and support spatially targeted interventions.\nDuring the Hackathon, participants will utilise Meta‚Äôs ‚ÄúFacebook Movement During Crisis‚Äù and ‚ÄúFacebook Population During Crisis‚Äù data derived from mobile phone location records, alongside IOM‚Äôs displacement data.\n\nThe Facebook Movement dataset records the movements of Facebook users, showing how people travel between different areas during a crisis. It includes both the typical movement patterns observed before the crisis and the actual movements during the event, allowing for a comparison between the two.\nThe Population dataset includes information about the number of active Facebook users in a specific area during a crisis, the typical number of active users in that area before the crisis, and the difference between these two values.\n\nBy integrating these datasets with traditional sources and socio-economic variables, teams will explore innovative approaches to enhance disaster response, optimise resource allocation, and develop predictive models for future crises.",
    "crumbs": [
      "Hackathon"
    ]
  },
  {
    "objectID": "workshop.html#day-2---may-22nd-2025",
    "href": "workshop.html#day-2---may-22nd-2025",
    "title": "Workshop",
    "section": "Day 2 - May 22nd 2025",
    "text": "Day 2 - May 22nd 2025\n\n9:00‚Äì9:30\n\nRecap of Hackathon Objectives and Goals - Prof.¬†Francisco Rowe\n\n\n\n9:00‚Äì11:00\n\nHackathon Session 2\n\n\n10:45‚Äì11:00 ‚Äì Coffee Break\n\n\n\n11:00‚Äì13:00\n\nHackathon Session 3\n\n\n\n13:00‚Äì14:00\nWorking Lunch\n\n\n14:00‚Äì17:00\n\nHackathon Final Session\n\n\n\n17:00‚Äì18:00\n\nTeam Presentations to Panel of Judges",
    "crumbs": [
      "Workshop"
    ]
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "International Organization for Migration (IOM), Dec 30 2024. DTM Pakistan ‚Äì Flood Response Community Needs Identification (CNI) | Round 6 ‚Äì Balochistan and Khyber Pakhtunkhwa Provinces (August 2024). IOM, Pakistan.\nIradukunda, R., Rowe, F., & Pietrostefani, E. (2025). Producing population-level estimates of internal displacement in Ukraine using GPS mobile phone data. arXiv preprint arXiv:2504.00003.\nOtto, F. E., Zachariah, M., Saeed, F., Siddiqi, A., Kamil, S., Mushtaq, H., ‚Ä¶ & Clarke, B. (2023). Climate change increased extreme monsoon rainfall, flooding highly vulnerable communities in Pakistan. Environmental Research: Climate, 2(2), 025001.\nRowe, F. (2022). Using digital footprint data to monitor human mobility and support rapid humanitarian responses. Regional Studies, Regional Science, 9(1), 665-668.\nWaleed, M., & Sajjad, M. (2025). High-resolution flood susceptibility mapping and exposure assessment in Pakistan: An integrated artificial intelligence, machine learning and geospatial framework. International Journal of Disaster Risk Reduction, 121(10544), 2.",
    "crumbs": [
      "References"
    ]
  },
  {
    "objectID": "dataaccess.html#working-in-snowflake",
    "href": "dataaccess.html#working-in-snowflake",
    "title": "Data Access",
    "section": "Working in Snowflake",
    "text": "Working in Snowflake\nOnce you have access to your Snowflake account, to set up your Snowflake Notebook, follow the following steps:\n\nIn the Home page, where you land when you log in, select + Create &gt; Notebook &gt; New Notebook\n\n\n\n\n\nIn the pop up window, select the following options, then hit Create\n\n\n\n\n\nIn the Databases tab, you‚Äôll find all the datasets available to you inside the PAKISTAN_FLOODS_2022_DATA database. Note the Schemas align to the dataset categories shared on the Data page\n\n\n\n\n\nNavigate to the top right Packages dropdown to install your favourite Python packages from the Snowflake Conda Channel for you to use\n\n\n\n\n\nYour Snowflake Notebook is ready to go! Create Markdown, SQL or Python cells to support you on your mission!",
    "crumbs": [
      "Snowflake",
      "Data Access"
    ]
  },
  {
    "objectID": "dataaccess.html#working-locally",
    "href": "dataaccess.html#working-locally",
    "title": "Data Access",
    "section": "Working locally",
    "text": "Working locally\nIf you prefer to work locally, you can use this Jupyter Notebook that has the information needed for you to connect to your Snowflake account from your laptop.",
    "crumbs": [
      "Snowflake",
      "Data Access"
    ]
  }
]